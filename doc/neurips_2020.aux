\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\BKM@entry[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\BKM@entry{id=1,dest={73656374696F6E2E31},srcline={89}}{496E74726F64756374696F6E}
\citation{russo}
\citation{xu,negrea}
\citation{xu,bu}
\citation{cotnrol}
\citation{mine}
\citation{control}
\citation{book,measures}
\citation{variational}
\citation{mine,visual,cpc}
\citation{cpcv2}
\citation{variational}
\citation{variational}
\citation{variational}
\babel@aux{british}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\BKM@entry{id=2,dest={73656374696F6E2E32},srcline={104}}{52656C6174656420576F726B}
\citation{variational,mine,infomax,cpc,cpcv2}
\citation{mine}
\citation{infomax}
\citation{cpc}
\citation{cpcv2}
\citation{variational}
\citation{variational}
\citation{russo}
\citation{dv,book}
\citation{overfit}
\citation{xu,bu}
\citation{negrea}
\citation{kuzborskij}
\citation{bu,sgld}
\citation{russo,zu}
\citation{haghifam}
\citation{steinke}
\citation{negrea}
\citation{control}
\citation{control}
\citation{measures}
\citation{control}
\citation{visual}
\citation{infomax}
\citation{cpc}
\citation{cpc}
\citation{cpcv2}
\citation{moco,mocov2}
\citation{simclr}
\citation{simclrv2}
\citation{visual}
\citation{visual}
\BKM@entry{id=3,dest={73656374696F6E2E33},srcline={111}}{5072656C696D696E6172696573}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\citation{kl}
\citation{cover}
\BKM@entry{id=4,dest={73656374696F6E2E34},srcline={128}}{5768656E20446F20426F756E64732048757274204C6561726E696E673F}
\citation{variational}
\citation{cpc,variational}
\citation{variational}
\citation{variational}
\citation{variational}
\@writefile{toc}{\contentsline {section}{\numberline {3}Preliminaries}{3}{section.3}\protected@file@percent }
\newlabel{eq:inf}{{1}{3}{Preliminaries}{equation.3.1}{}}
\newlabel{eq:infsimp}{{2}{3}{Preliminaries}{equation.3.2}{}}
\newlabel{eq:phi}{{3}{3}{Preliminaries}{equation.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}When Do Bounds Hurt Learning?}{3}{section.4}\protected@file@percent }
\BKM@entry{id=5,dest={73656374696F6E2E35},srcline={148}}{566172696174696F6E616C20426F756E647320666F722047656E6572616C697A6174696F6E}
\BKM@entry{id=6,dest={73756273656374696F6E2E352E31},srcline={152}}{4C6561726E696E6720566172696174696F6E616C20426F756E6473}
\citation{mine}
\citation{control}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Left} Conventional lose bounds suffer from high bias which hurts generalization of the learnt distribution, \textbf  {Center} Learnt distribution is additionally hampered with noisy approximations, \textbf  {Right} Biased estimates in conjunction with noisy dynamics hurt the completeness of learnt distribution.\relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dist}{{1}{4}{\textbf {Left} Conventional lose bounds suffer from high bias which hurts generalization of the learnt distribution, \textbf {Center} Learnt distribution is additionally hampered with noisy approximations, \textbf {Right} Biased estimates in conjunction with noisy dynamics hurt the completeness of learnt distribution.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Variational Bounds for Generalization}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Learning Variational Bounds}{4}{subsection.5.1}\protected@file@percent }
\newlabel{eq:mine}{{4}{4}{Learning Variational Bounds}{equation.5.4}{}}
\newlabel{eq:monte}{{5}{4}{Learning Variational Bounds}{equation.5.5}{}}
\newlabel{eq:nce}{{6}{4}{Learning Variational Bounds}{equation.5.6}{}}
\newlabel{eq:non}{{7}{4}{Learning Variational Bounds}{equation.5.7}{}}
\citation{control}
\citation{russo}
\citation{control}
\citation{russo}
\citation{control}
\BKM@entry{id=7,dest={73756273656374696F6E2E352E32},srcline={186}}{496D70726F76696E6720426F756E6473206F6E204D49}
\citation{banach,contraction}
\citation{contraction}
\citation{sql}
\citation{banach}
\newlabel{eq:expcum}{{8}{5}{Learning Variational Bounds}{equation.5.8}{}}
\newlabel{eq:bound}{{9}{5}{Learning Variational Bounds}{equation.5.9}{}}
\newlabel{eq:impbound}{{10}{5}{Learning Variational Bounds}{equation.5.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Improving Bounds on MI}{5}{subsection.5.2}\protected@file@percent }
\newlabel{one}{{1}{5}{}{innercustomgeneric.1}{}}
\newlabel{eq:newbound}{{11}{5}{}{equation.5.11}{}}
\citation{contraction}
\citation{mellowmax}
\citation{sql,emix}
\BKM@entry{id=8,dest={73656374696F6E2E36},srcline={229}}{4578706572696D656E7473}
\newlabel{eq:contraction}{{12}{6}{Improving Bounds on MI}{equation.5.12}{}}
\newlabel{two}{{2}{6}{}{innercustomgeneric.2}{}}
\newlabel{prop3}{{3}{6}{}{innercustomgeneric.3}{}}
\newlabel{eq:novelbound}{{13}{6}{}{equation.5.13}{}}
\newlabel{novelbound}{{3}{6}{}{equation.5.13}{}}
\newlabel{eq:main}{{14}{6}{Improving Bounds on MI}{equation.5.14}{}}
\newlabel{prop4}{{4}{6}{}{innercustomgeneric.4}{}}
\BKM@entry{id=9,dest={73756273656374696F6E2E362E31},srcline={233}}{5365747570}
\citation{visual}
\citation{infomax}
\BKM@entry{id=10,dest={73756273656374696F6E2E362E32},srcline={237}}{556E7375706572766973656420496E7374616E6365204469736372696D696E6174696F6E}
\citation{variational}
\citation{variational}
\citation{infomax}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Setup}{7}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Unsupervised Instance Discrimination}{7}{subsection.6.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of results based on Generalization Error (Gen. Error) and Top1 Accuracy (Acc) for all objectives on standard and large-scale benchmarks using ResNet-18 and ResNet-34 models. Highlighted entries depict best performing objectives in the unsupervised setting. $\phi $-divergence measures such as JSD and RKL depict improved generalization and performance comparable to conventional MI-based InfoNCE and fully-supervised CE. The DV objective on the other hand, demonstrates instability as a result of exponential estimates. For complete results on bias and variance refer to Appendix.\relax }}{7}{table.caption.3}\protected@file@percent }
\newlabel{tab:table}{{1}{7}{Summary of results based on Generalization Error (Gen. Error) and Top1 Accuracy (Acc) for all objectives on standard and large-scale benchmarks using ResNet-18 and ResNet-34 models. Highlighted entries depict best performing objectives in the unsupervised setting. $\phi $-divergence measures such as JSD and RKL depict improved generalization and performance comparable to conventional MI-based InfoNCE and fully-supervised CE. The DV objective on the other hand, demonstrates instability as a result of exponential estimates. For complete results on bias and variance refer to Appendix.\relax }{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Gen. Error for variational objectives on standard and large-scale benchmarks for the ResNet-18 model. $\phi $-divergence measures demonstrate improved stability and generalization in comparison to conventional InfoNCE and fully-supervised CE wh