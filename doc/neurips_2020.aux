\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\BKM@entry[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\BKM@entry{id=1,dest={73656374696F6E2E31},srcline={89}}{496E74726F64756374696F6E}
\citation{russo}
\citation{xu,negrea}
\citation{xu,bu}
\citation{cotnrol}
\citation{mine}
\citation{control}
\citation{book,measures}
\citation{variational}
\citation{mine,visual,cpc}
\citation{cpcv2}
\citation{variational}
\citation{variational}
\citation{variational}
\babel@aux{british}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\BKM@entry{id=2,dest={73656374696F6E2E32},srcline={104}}{52656C6174656420576F726B}
\citation{variational,mine,infomax,cpc,cpcv2}
\citation{mine}
\citation{infomax}
\citation{cpc}
\citation{cpcv2}
\citation{variational}
\citation{variational}
\citation{russo}
\citation{dv,book}
\citation{overfit}
\citation{xu,bu}
\citation{negrea}
\citation{kuzborskij}
\citation{bu,sgld}
\citation{russo,zu}
\citation{haghifam}
\citation{steinke}
\citation{negrea}
\citation{control}
\citation{control}
\citation{measures}
\citation{control}
\citation{visual}
\citation{infomax}
\citation{cpc}
\citation{cpc}
\citation{cpcv2}
\citation{moco,mocov2}
\citation{simclr}
\citation{simclrv2}
\citation{visual}
\citation{visual}
\BKM@entry{id=3,dest={73656374696F6E2E33},srcline={111}}{5072656C696D696E6172696573}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\citation{kl}
\citation{cover}
\BKM@entry{id=4,dest={73656374696F6E2E34},srcline={128}}{5768656E20446F20426F756E64732048757274204C6561726E696E673F}
\citation{variational}
\citation{cpc,variational}
\citation{variational}
\citation{variational}
\citation{variational}
\@writefile{toc}{\contentsline {section}{\numberline {3}Preliminaries}{3}{section.3}\protected@file@percent }
\newlabel{eq:inf}{{1}{3}{Preliminaries}{equation.3.1}{}}
\newlabel{eq:infsimp}{{2}{3}{Preliminaries}{equation.3.2}{}}
\newlabel{eq:phi}{{3}{3}{Preliminaries}{equation.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}When Do Bounds Hurt Learning?}{3}{section.4}\protected@file@percent }
\BKM@entry{id=5,dest={73656374696F6E2E35},srcline={148}}{566172696174696F6E616C20426F756E647320666F722047656E6572616C697A6174696F6E}
\BKM@entry{id=6,dest={73756273656374696F6E2E352E31},srcline={152}}{4C6561726E696E6720566172696174696F6E616C20426F756E6473}
\citation{mine}
\citation{control}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Left} Conventional lose bounds suffer from high bias which hurts generalization of the learnt distribution, \textbf  {Center} Learnt distribution is additionally hampered with noisy approximations, \textbf  {Right} Biased estimates in conjunction with noisy dynamics hurt the completeness of learnt distribution.\relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dist}{{1}{4}{\textbf {Left} Conventional lose bounds suffer from high bias which hurts generalization of the learnt distribution, \textbf {Center} Learnt distribution is additionally hampered with noisy approximations, \textbf {Right} Biased estimates in conjunction with noisy dynamics hurt the completeness of learnt distribution.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Variational Bounds for Generalization}{4}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Learning Variational Bounds}{4}{subsection.5.1}\protected@file@percent }
\newlabel{eq:mine}{{4}{4}{Learning Variational Bounds}{equation.5.4}{}}
\newlabel{eq:monte}{{5}{4}{Learning Variational Bounds}{equation.5.5}{}}
\newlabel{eq:nce}{{6}{4}{Learning Variational Bounds}{equation.5.6}{}}
\newlabel{eq:non}{{7}{4}{Learning Variational Bounds}{equation.5.7}{}}
\citation{control}
\citation{russo}
\citation{control}
\citation{russo}
\citation{control}
\BKM@entry{id=7,dest={73756273656374696F6E2E352E32},srcline={186}}{496D70726F76696E6720426F756E6473206F6E204D49}
\citation{banach,contraction}
\citation{contraction}
\citation{sql}
\citation{banach}
\newlabel{eq:expcum}{{8}{5}{Learning Variational Bounds}{equation.5.8}{}}
\newlabel{eq:bound}{{9}{5}{Learning Variational Bounds}{equation.5.9}{}}
\newlabel{eq:impbound}{{10}{5}{Learning Variational Bounds}{equation.5.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Improving Bounds on MI}{5}{subsection.5.2}\protected@file@percent }
\newlabel{one}{{1}{5}{}{innercustomgeneric.1}{}}
\newlabel{eq:newbound}{{11}{5}{}{equation.5.11}{}}
\citation{contraction}
\citation{mellowmax}
\citation{sql,emix}
\BKM@entry{id=8,dest={73656374696F6E2E36},srcline={229}}{4578706572696D656E7473}
\newlabel{eq:contraction}{{12}{6}{Improving Bounds on MI}{equation.5.12}{}}
\newlabel{two}{{2}{6}{}{innercustomgeneric.2}{}}
\newlabel{prop3}{{3}{6}{}{innercustomgeneric.3}{}}
\newlabel{eq:novelbound}{{13}{6}{}{equation.5.13}{}}
\newlabel{novelbound}{{3}{6}{}{equation.5.13}{}}
\newlabel{eq:main}{{14}{6}{Improving Bounds on MI}{equation.5.14}{}}
\newlabel{prop4}{{4}{6}{}{innercustomgeneric.4}{}}
\BKM@entry{id=9,dest={73756273656374696F6E2E362E31},srcline={233}}{5365747570}
\citation{visual}
\citation{infomax}
\BKM@entry{id=10,dest={73756273656374696F6E2E362E32},srcline={237}}{556E7375706572766973656420496E7374616E6365204469736372696D696E6174696F6E}
\citation{variational}
\citation{variational}
\citation{infomax}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Setup}{7}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Unsupervised Instance Discrimination}{7}{subsection.6.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Summary of results based on Generalization Error (Gen. Error) and Top1 Accuracy (Acc) for all objectives on standard and large-scale benchmarks using ResNet-18 and ResNet-34 models. Highlighted entries depict best performing objectives in the unsupervised setting. $\phi $-divergence measures such as JSD and RKL depict improved generalization and performance comparable to conventional MI-based InfoNCE and fully-supervised CE. The DV objective on the other hand, demonstrates instability as a result of exponential estimates. For complete results on bias and variance refer to Appendix.\relax }}{7}{table.caption.3}\protected@file@percent }
\newlabel{tab:table}{{1}{7}{Summary of results based on Generalization Error (Gen. Error) and Top1 Accuracy (Acc) for all objectives on standard and large-scale benchmarks using ResNet-18 and ResNet-34 models. Highlighted entries depict best performing objectives in the unsupervised setting. $\phi $-divergence measures such as JSD and RKL depict improved generalization and performance comparable to conventional MI-based InfoNCE and fully-supervised CE. The DV objective on the other hand, demonstrates instability as a result of exponential estimates. For complete results on bias and variance refer to Appendix.\relax }{table.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Gen. Error for variational objectives on standard and large-scale benchmarks for the ResNet-18 model. $\phi $-divergence measures demonstrate improved stability and generalization in comparison to conventional InfoNCE and fully-supervised CE which depict high bias. Conventional DV, on the other hand, presents high variance and unstable convergence as a result of exponential estimates of the partition function.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:plot}{{2}{7}{Gen. Error for variational objectives on standard and large-scale benchmarks for the ResNet-18 model. $\phi $-divergence measures demonstrate improved stability and generalization in comparison to conventional InfoNCE and fully-supervised CE which depict high bias. Conventional DV, on the other hand, presents high variance and unstable convergence as a result of exponential estimates of the partition function.\relax }{figure.caption.4}{}}
\citation{variational}
\BKM@entry{id=11,dest={73656374696F6E2E37},srcline={278}}{44697363757373696F6E}
\bibstyle{unsrt}
\bibdata{sample}
\bibcite{russo}{{1}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{8}{section.7}\protected@file@percent }
\bibcite{xu}{{2}{}{{}}{{}}}
\bibcite{negrea}{{3}{}{{}}{{}}}
\bibcite{bu}{{4}{}{{}}{{}}}
\bibcite{mine}{{5}{}{{}}{{}}}
\bibcite{control}{{6}{}{{}}{{}}}
\bibcite{book}{{7}{}{{}}{{}}}
\bibcite{measures}{{8}{}{{}}{{}}}
\bibcite{variational}{{9}{}{{}}{{}}}
\bibcite{visual}{{10}{}{{}}{{}}}
\bibcite{cpc}{{11}{}{{}}{{}}}
\bibcite{cpcv2}{{12}{}{{}}{{}}}
\bibcite{infomax}{{13}{}{{}}{{}}}
\bibcite{dv}{{14}{}{{}}{{}}}
\bibcite{overfit}{{15}{}{{}}{{}}}
\bibcite{kuzborskij}{{16}{}{{}}{{}}}
\bibcite{sgld}{{17}{}{{}}{{}}}
\bibcite{haghifam}{{18}{}{{}}{{}}}
\bibcite{steinke}{{19}{}{{}}{{}}}
\bibcite{moco}{{20}{}{{}}{{}}}
\bibcite{mocov2}{{21}{}{{}}{{}}}
\bibcite{simclr}{{22}{}{{}}{{}}}
\bibcite{simclrv2}{{23}{}{{}}{{}}}
\bibcite{kl}{{24}{}{{}}{{}}}
\bibcite{cover}{{25}{}{{}}{{}}}
\bibcite{banach}{{26}{}{{}}{{}}}
\bibcite{contraction}{{27}{}{{}}{{}}}
\bibcite{sql}{{28}{}{{}}{{}}}
\bibcite{mellowmax}{{29}{}{{}}{{}}}
\bibcite{emix}{{30}{}{{}}{{}}}
\BKM@entry{id=12,dest={617070656E6469782E41},srcline={288}}{50726F6F6673}
\citation{control}
\@writefile{toc}{\contentsline {section}{\numberline {A}Proofs}{11}{appendix.A}\protected@file@percent }
\newlabel{eq:case1}{{16}{11}{Proofs}{equation.A.16}{}}
\newlabel{eq:case2}{{17}{11}{Proofs}{equation.A.17}{}}
\BKM@entry{id=13,dest={617070656E6469782E42},srcline={367}}{4164646974696F6E616C20526573756C7473}
\BKM@entry{id=14,dest={73756273656374696F6E2E422E31},srcline={368}}{5265734E65742D3138}
\@writefile{toc}{\contentsline {section}{\numberline {B}Additional Results}{12}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}ResNet-18}{12}{subsection.B.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Gen. Error for variational objectives on standard and large-scale benchmarks for the ResNet-18 model. $\phi $-divergence measures demonstrate improved stability and generalization in comparison to conventional InfoNCE and fully-supervised CE which depict high bias. Conventional DV, on the other hand, presents high variance and unstable convergence as a result of exponential estimates of the partition function.\relax }}{12}{figure.caption.6}\protected@file@percent }
\BKM@entry{id=15,dest={73756273656374696F6E2E422E32},srcline={385}}{5265734E65742D3334}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison of Top1 classification accuracies for variational objectives on standard and large-scale benchmarks for the ResNet-18 model. While fully-supervised CE demonstrates best performance, InfoNCE and $\phi $-divergence measures depict comparably accurate performance. Conventional DV, on the other hand, demonstrates a failure to learn.\relax }}{13}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of Top5 classification accuracies for variational objectives on standard and large-scale benchmarks for the ResNet-18 model. Performance is analogous to Top1 counterpart with variational bounds demonstrating improved performance on standard mnist and fashionmnist benchmarks.\relax }}{13}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}ResNet-34}{13}{subsection.B.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Gen. Error for variational objectives on standard and large-scale benchmarks for the ResNet-34 model. $\phi $-divergence measures scale to well to the deeper ResNet-34 model and maintain minimum bias in generalization.\relax }}{13}{figure.caption.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of Top1 classification accuracies for variational objectives on standard and large-scale benchmarks for the ResNet-34 model. Bounds tend to overfit in the case of EMNIST dataset as a result of large (26) number of classes.\relax }}{13}{figure.caption.10}\protected@file@percent }
\BKM@entry{id=16,dest={617070656E6469782E43},srcline={402}}{496D706C656D656E746174696F6E2044657461696C73}
\BKM@entry{id=17,dest={73756273656374696F6E2E432E31},srcline={404}}{4E6F7465206F6E204578706572696D656E7473}
\citation{visual}
\citation{infomax}
\BKM@entry{id=18,dest={73756273656374696F6E2E432E32},srcline={407}}{4879706572706172616D6574657273}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparison of Top5 classification accuracies for variational objectives on standard and large-scale benchmarks for the ResNet-34 model. Variational bounds demonstrate comparable performance to fully-supervised CE on MNIST, FashionMNIST and EMNIST benchmarks.\relax }}{14}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Implementation Details}{14}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Note on Experiments}{14}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Hyperparameters}{14}{subsection.C.2}\protected@file@percent }
\gdef \@abspage@last{14}
